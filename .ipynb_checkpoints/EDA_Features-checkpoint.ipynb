{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "#import folium\n",
    "\n",
    "# Visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Saving models\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', 150,\n",
    "             'display.max_rows', 150)\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, RepeatedKFold, train_test_split, cross_validate, cross_val_score \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error, median_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn import ensemble\n",
    "\n",
    "import requests, json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "\n",
    "%autosave 60\n",
    "import cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data organized\n",
    "    Initialily to organize the data I rewrote the order_products grouping by order_id. Thus, the full data after\n",
    "    the manipulation will be organized by the order_id as variable key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "   * Dataset order_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path0 = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products = pd.read_csv(os.path.join(path0, \"order_products.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables to order_products dataset\n",
    "   Here I use the **order_products_var** function to create new variables:\n",
    "   * quantity_UN: Quantity of products in units.\n",
    "   * quantity_KG: Quantity of products in KG.\n",
    "   \n",
    "After create these variables I agroup the dataset by order_id and create a other new variables:\n",
    "   * UN_plus_KG: Quantity of units plus Quantity of KG\n",
    "   * UN_mult_KG: Quantity of units times Quantity of KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products = order_products_var(order_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dataset orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(os.path.join(path0, \"orders.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here I rename the lat and long variables to indentify the end of the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders.rename({'lat': 'lat_destination', 'lng': 'lng_destination'}, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the datasets (orders and order_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(orders, order_products, how='inner', on='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Dataset shoppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers = pd.read_csv(os.path.join(path0, \"shoppers.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treatment of missing in shoppers dataset\n",
    "   The found_rate, accepted_rate and rating variables have missing. To treat this I filter the dataframe taking just qhere found_rate is null then I find the values to picking_speed in the new dataframe. With these values I create a new dataframe making a filter in original dataframe using the picking_speed values founded, **found_rate2** dataframe. After that I fill the found_rate missing using the mode of **found_rate2** dataframe.\n",
    "   \n",
    "   The missing value of accepted_rate and rating variabel were filled using the mode of column in original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rate = shoppers[shoppers.found_rate.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rate.picking_speed.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rate2 = shoppers[(shoppers.picking_speed == 2.41) | (shoppers.picking_speed == 2.19) | (shoppers.picking_speed == 2.33)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers.loc[shoppers['found_rate'].isnull(), 'found_rate'] = found_rate2['found_rate'].mode().iat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers.loc[shoppers['accepted_rate'].isnull(), 'accepted_rate'] = shoppers['accepted_rate'].mode().iat[0]\n",
    "shoppers.loc[shoppers['rating'].isnull(), 'rating'] = shoppers['rating'].mode().iat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoppers.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the datasets (full and shoppers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(full, shoppers, how='left', on='shopper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Dataset storebranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storebranch = pd.read_csv(os.path.join(path0, \"storebranch.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here I rename the lat and long variables to indentify the beginning of the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storebranch = storebranch.rename({'lat': 'lat_origin', 'lng': 'lng_origin'}, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storebranch.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storebranch.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storebranch.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the datasets (full and storebranch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(full, storebranch, how='left', on='store_branch_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = time_sep(full, col = 'promised_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between origin and destiny and car path\n",
    "The datasets offered have lat and long to origin and destiny points. Using these information I calculate the Haversine distance:\n",
    "   * The Haversine calculator computes the distance between two points on a spherical model of the Earth along a great circle arc.\n",
    "   \n",
    "I calculate too the path by car between the coordenates using the project-osrm project API.\n",
    "\n",
    "The Open Source Routing Machine or OSRM is a C++ implementation of a high-performance routing engine for shortest paths in road networks. Licensed under the permissive 2-clause BSD license, OSRM is a free network service.\n",
    "\n",
    "It combines sophisticated routing algorithms with the open and free road network data of the OpenStreetMap (OSM) project. Shortest path computation on a continental sized network can take up to several seconds if it is done without a so-called speedup-technique. OSRM uses an implementation of contraction hierarchies and is able to compute and output a shortest path between any origin and destination within a few milliseconds, whereby the pure route computation takes much less time. Most effort is spent in annotating the route and transmitting the geometry over the network.\n",
    "\n",
    "   * **distance**: The distance traveled by the route, in float meters.\n",
    "   * **duration**: The estimated travel time, in float number of seconds.\n",
    "   * **weight**: The calculated weight of the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['distance_haversine'] = distance(full.lat_origin, full.lng_origin, full.lat_destination, full.lng_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full[['lng_origin', 'lat_origin', 'lng_destination', 'lat_destination']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full['distance_car'], full['weight_car'], full['duration'] = path(full, 'lng_origin', 'lat_origin', 'lng_destination', 'lat_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv(os.path.join(path0, \"full_distance.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['city_origin'], full['state_origin'], full['county_origin'], full['neighbourhood_origin'] = location(full, 'lat_origin', 'lng_origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv(os.path.join(path0, \"full_origin.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['city_destiny'], full['state_destiny'], full['county_destiny'], full['neighbourhood_destiny'] = location(full, 'lat_destination', 'lng_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv(os.path.join(path0, \"full_destiny.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.loc[full['city_origin'] == full['city_destiny'], 'same_city'] = 1 \n",
    "full.loc[full['city_origin'] != full['city_destiny'], 'same_city'] = 0\n",
    "\n",
    "full.loc[full['state_origin'] == full['state_destiny'], 'same_state'] = 1 \n",
    "full.loc[full['state_origin'] != full['state_destiny'], 'same_state'] = 0\n",
    "\n",
    "full.loc[full['county_origin'] == full['county_destiny'], 'same_county'] = 1 \n",
    "full.loc[full['county_origin'] != full['county_destiny'], 'same_county'] = 0\n",
    "\n",
    "full.loc[full['neighbourhood_origin'] == full['neighbourhood_destiny'], 'same_neighbourhood'] = 1 \n",
    "full.loc[full['neighbourhood_origin'] != full['neighbourhood_destiny'], 'same_neighbourhood'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.loc[full['city_origin'].isnull(), 'city_origin'] = 'undefined'\n",
    "full.loc[full['city_destiny'].isnull(), 'city_destiny'] = 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['path_city'] = full['city_origin'] + '_X_' + full['city_destiny']\n",
    "full['path_state'] = full['state_origin'] + '_X_' + full['state_destiny']\n",
    "full['path_county'] = full['county_origin'] + '_X_' + full['county_destiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop = full.groupby('store_id').aggregate(\n",
    "                            {'shopper_id': 'count'\n",
    "                            }).reset_index()\n",
    "shop = shop.rename({'shopper_id': 'shoppers_number'}, axis=1)  \n",
    "\n",
    "full2 = pd.merge(full, shop, how='left', on='store_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = full2.groupby('store_id').aggregate(\n",
    "                            {'store_branch_id': 'count'\n",
    "                            }).reset_index()\n",
    "branch = branch.rename({'store_branch_id': 'store_branch_number'}, axis=1)  \n",
    "\n",
    "full2 = pd.merge(full2, branch, how='left', on='store_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full2.to_csv(os.path.join(path0, \"full_new.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
